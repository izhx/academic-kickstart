---
title: "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus"
authors:
- Hao Fei
- zhangmeishan
- Donghong Ji
author_notes:
- ""
- "Corresponding"

date: "2020-04-06T00:00:00Z"

# Schedule page publish date (NOT publication's date).
publishDate: "2020-04-06T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: In *The 58th Annual Meeting of the Association for Computational Linguistics*
publication_short: In *ACL 2020*

abstract: "Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly."

# Summary. An optional shortened abstract.
# summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.

tags:
- ACL
- SRL
categories:
- Paper
featured: true

links:
# - name: Custom Link
#   url: http://example.org
url_pdf: "https://arxiv.org/pdf/2004.06295v2.pdf"
url_code: "https://arxiv.org/pdf/2004.06295v2.pdf"

---
